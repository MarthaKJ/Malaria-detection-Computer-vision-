# -*- coding: utf-8 -*-
"""villgrodataset (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B_ozimFocuG5Fm_gyOEIcmpMidkpWnSu

# TRAINING WITH THE YOLOV8n MODEL
"""

!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="QgekeJr37oDZWfaaKE6o")
project = rf.workspace("ai-for-microscopy-diagnosis").project("villgro-bsdf5")
version = project.version(8)
dataset = version.download("yolov8")

#Exploring the villgro dataset
!pip install opencv-python matplotlib

!pip install ultralytics
#installing the YOLOv8 model
from ultralytics import YOLO
import os
from IPython.display import display, Image
from IPython import display
display.clear_output()
!yolo mode = checks

"""Training the model"""

#Training on custom dataset
!yolo task = detect  mode = train model = yolov8n.pt data = /content/Villgro-8/data.yaml epochs =20

#loading the model trained
model = YOLO('/content/runs/detect/train/weights/best.pt')
#validating the model
metric = model.val(data = '/content/Villgro-8/data.yaml')

#testing with the villgro test dataset
import cv2
# # Define the path to the trained model
model_path = '/content/runs/detect/train/weights/best.pt'
# Load the YOLOv8 model
model = YOLO(model_path)

# path to the test dataset
test_images_path = '/content/Villgro-8/test/images'

test_images = [os.path.join(test_images_path, img) for img in os.listdir(test_images_path) if img.endswith('.jpg')]
# Run inference on the test dataset
results = []
for image_path in test_images:
    # Load image
    img = cv2.imread(image_path)

    # Run inference
    result = model(img)

"""testing with the ocular dataset"""

# evaluating ocular dataset
!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="QgekeJr37oDZWfaaKE6o")
project = rf.workspace("ai-for-microscopy-diagnosis").project("ocular-sx12d")
version = project.version(4)
dataset = version.download("yolov8")

#loading the model trained
model = YOLO('/content/runs/detect/train/weights/best.pt')
#validating the model
metric = model.val(data = '/content/Ocular-4/data.yaml')

import cv2
# # Define the path to the trained model
model_path = '/content/runs/detect/train/weights/best.pt'
# Load the YOLOv8 model
model = YOLO(model_path)

# path to the test dataset
test_images_path = '/content/Ocular-4/test/images'

test_images = [os.path.join(test_images_path, img) for img in os.listdir(test_images_path) if img.endswith('.jpg')]
# Run inference on the test dataset
results = []
for image_path in test_images:
    # Load image
    img = cv2.imread(image_path)

    # Run inference
    result = model(img)

"""Evaluating with the air-1 dataset"""

!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="gehjSWtCf9BsWy2BOqAV")
project = rf.workspace("dsi-oiyah").project("air-aco1r")
version = project.version(1)
dataset = version.download("yolov8")

#loading the model trained
model = YOLO('/content/runs/detect/train/weights/best.pt')
#validating the model
metric = model.val(data = '/content/air-1/data.yaml')

#testing with the air-1 test dataset
import cv2
# # Define the path to the trained model
model_path = '/content/runs/detect/train/weights/best.pt'
# Load the YOLOv8 model
model = YOLO(model_path)

# path to the test dataset
test_images_path = '/content/air-1/test/images'

test_images = [os.path.join(test_images_path, img) for img in os.listdir(test_images_path) if img.endswith('.jpg')]
# Run inference on the test dataset
results = []
for image_path in test_images:
    # Load image
    img = cv2.imread(image_path)

    # Run inference
    result = model(img)

"""evaluating with the airphone dataset"""

!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="gehjSWtCf9BsWy2BOqAV")
project = rf.workspace("dsi-oiyah").project("air-phone")
version = project.version(1)
dataset = version.download("yolov8")

#loading the model trained
model = YOLO('/content/runs/detect/train/weights/best.pt')
#validating the model
metric = model.val(data = '/content/air-phone-1/data.yaml')

#testing with the air-phone-1 test dataset
import cv2
# # Define the path to the trained model
model_path = '/content/runs/detect/train/weights/best.pt'
# Load the YOLOv8 model
model = YOLO(model_path)

# path to the test dataset
test_images_path = '/content/air-phone-1/test/images'

test_images = [os.path.join(test_images_path, img) for img in os.listdir(test_images_path) if img.endswith('.jpg')]
# Run inference on the test dataset
results = []
for image_path in test_images:
    # Load image
    img = cv2.imread(image_path)

    # Run inference
    result = model(img)

"""comparing results
Villgro Dataset:                      
Precision (P): 0.558
Recall (R): 0.657
mAP50: 0.651
mAP50-95: 0.355

Ocular Dataset:
Precision (P): 0.557
Recall (R): 0.629
mAP50: 0.610
mAP50-95: 0.357

Air-1 Dataset:
Precision (P): 0.0411
Recall (R): 0.135
mAP50: 0.0234
mAP50-95: 0.00409

Air-Phone Dataset:
Precision (P): 0.0163
Recall (R): 0.218
mAP50: 0.011
mAP50-95: 0.00169

Best Performance: The model performed best on the Villgro Dataset, followed closely by the Ocular Dataset. These datasets showed the highest overall metrics for precision, recall, and mAP.

Poor Performance: The model struggled significantly with the Air-1 and Air-Phone datasets, where it exhibited notably lower precision, recall, and mAP scores. This indicates challenges in generalizing to different data distributions or characteristics present in these datasets.


"""



"""# TRAINING  WITH THE YOLOV8M MODEL"""

!yolo task = detect  mode = train model = yolov8m.pt data =/content/Villgro-8/data.yaml epochs =40

"""# Validation

with airphone dataset
"""

#loading the model trained
model = YOLO('/content/runs/detect/train2/weights/best.pt')
#validating the model
metric = model.val(data = '/content/air-phone-1/data.yaml')

"""with air1 data"""

#loading the model trained
model = YOLO('/content/runs/detect/train2/weights/best.pt')
#validating the model
metric = model.val(data = '/content/air-1/data.yaml')

"""with ocular data"""

#loading the model trained
model = YOLO('/content/runs/detect/train2/weights/best.pt')
#validating the model
metric = model.val(data = '/content/Ocular-4/data.yaml')

!zip -r /content/runs.zip /content/runs

from google.colab import drive
drive.mount('/content/drive')

!cp -r /content/runs /content/drive/MyDrive/villgro

"""# Testing what the model is seeing and what we are seeing side by side"""

from ultralytics import YOLO
import cv2
import os
import matplotlib.pyplot as plt

# loading the trained model
model = YOLO('/content/runs/detect/train2/weights/best.pt')

# function to process and display the images
def process_and_display_images(image_folder, num_images=10):
    images = os.listdir(image_folder)[:num_images]

    for img_name in images:
        img_path = os.path.join(image_folder, img_name)

        # Read the original image
        original_img = cv2.imread(img_path)
        original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)

        # Run inference
        results = model(img_path)

        # Plot results
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))

        ax1.imshow(original_img)
        ax1.set_title('Original Image')
        ax1.axis('off')

        ax2.imshow(results[0].plot())
        ax2.set_title('Model Prediction')
        ax2.axis('off')

        plt.tight_layout()
        plt.show()

image_folder = '/content/Villgro-8/test/images'
process_and_display_images(image_folder)

"""Grround truth vs predicted bounding boxes"""

import os
import matplotlib.pyplot as plt
import cv2
import numpy as np
from ultralytics import YOLO

def load_yolo_annotations(annotation_path, image_width, image_height):
    boxes = []
    with open(annotation_path, 'r') as f:
        for line in f:
            class_id, x_center, y_center, width, height = map(float, line.strip().split())
            x = int((x_center - width / 2) * image_width)
            y = int((y_center - height / 2) * image_height)
            w = int(width * image_width)
            h = int(height * image_height)
            boxes.append([x, y, w, h])
    return boxes

def draw_boxes(img, boxes, color):
    for box in boxes:
        x, y, w, h = [int(v) for v in box]
        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)
    return img

def process_and_display_images(test_folder, model, num_images=10):
    image_folder = os.path.join(test_folder, 'images')
    label_folder = os.path.join(test_folder, 'labels')

    images = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))][:num_images]

    for img_name in images:
        img_path = os.path.join(image_folder, img_name)
        label_name = os.path.splitext(img_name)[0] + '.txt'
        label_path = os.path.join(label_folder, label_name)

        # Read the original image
        original_img = cv2.imread(img_path)
        original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
        height, width = original_img.shape[:2]

        # Load ground truth boxes
        gt_boxes = load_yolo_annotations(label_path, width, height)

        # Run inference
        results = model(img_path)

        # Plot results
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))

        # Draw ground truth boxes
        gt_img = original_img.copy()
        gt_img = draw_boxes(gt_img, gt_boxes, (0, 255, 0))  # Green for ground truth
        ax1.imshow(gt_img)
        ax1.set_title('Ground Truth')
        ax1.axis('off')

        # Use the plot method from results
        ax2.imshow(results[0].plot())
        ax2.set_title('Model Prediction')
        ax2.axis('off')

        plt.tight_layout()
        plt.show()

        print(f"Displayed comparison for image: {img_name}")

# Load your trained model
model = YOLO('/content/runs/detect/train2/weights/best.pt')

# Directory containing your test images and labels folders
test_dir = '/content/Villgro-8/test'

# Process and display images
process_and_display_images(test_dir, model)

print("All comparisons have been displayed.")

"""Ground truth vs predicted bounding boxes but highlighting the additional predicted bounding boxes"""

def calculate_iou(box1, box2):
    # Convert YOLO format to [x1, y1, x2, y2]
    b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[0] + box1[2], box1[1] + box1[3]
    b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[0] + box2[2], box2[1] + box2[3]

    # Calculate intersection area
    inter_x1 = max(b1_x1, b2_x1)
    inter_y1 = max(b1_y1, b2_y1)
    inter_x2 = min(b1_x2, b2_x2)
    inter_y2 = min(b1_y2, b2_y2)
    inter_area = max(0, inter_x2 - inter_x1) * max(0, inter_y2 - inter_y1)

    # Calculate union area
    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
    union_area = b1_area + b2_area - inter_area

    # Calculate IoU
    iou = inter_area / union_area if union_area > 0 else 0
    return iou

def highlight_additional_predictions(test_folder, model, iou_threshold=0.5, num_images=10):
    image_folder = os.path.join(test_folder, 'images')
    label_folder = os.path.join(test_folder, 'labels')

    images = [f for f in os.listdir(image_folder) if f.endswith(('.jpg', '.png', '.jpeg'))][:num_images]

    for img_name in images:
        img_path = os.path.join(image_folder, img_name)
        label_name = os.path.splitext(img_name)[0] + '.txt'
        label_path = os.path.join(label_folder, label_name)

        # Read the original image
        original_img = cv2.imread(img_path)
        original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
        height, width = original_img.shape[:2]

        # Load ground truth boxes
        gt_boxes = load_yolo_annotations(label_path, width, height)

        # Run inference
        results = model(img_path)
        pred_boxes = results[0].boxes.xywh.cpu().numpy()  # [x_center, y_center, width, height]

        # Convert pred_boxes to [x, y, w, h] format
        pred_boxes[:, 0] -= pred_boxes[:, 2] / 2
        pred_boxes[:, 1] -= pred_boxes[:, 3] / 2

        # Identify additional predictions
        additional_preds = []
        for pred_box in pred_boxes:
            matched = False
            for gt_box in gt_boxes:
                if calculate_iou(gt_box, pred_box) > iou_threshold:
                    matched = True
                    break
            if not matched:
                additional_preds.append(pred_box)

        # Plot results
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))

        # Ground Truth
        gt_img = original_img.copy()
        for box in gt_boxes:
            x, y, w, h = [int(v) for v in box]
            cv2.rectangle(gt_img, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green for ground truth
        ax1.imshow(gt_img)
        ax1.set_title('Ground Truth')
        ax1.axis('off')

        # Predictions with highlighted additional boxes
        pred_img = results[0].plot()
        for box in additional_preds:
            x, y, w, h = [int(v) for v in box]
            cv2.rectangle(pred_img, (x, y), (x + w, y + h), (0, 0, 255), 3)  # Red for additional predictions
        ax2.imshow(pred_img)
        ax2.set_title('Model Prediction (Red: Additional Predictions)')
        ax2.axis('off')

        plt.tight_layout()
        plt.show()

        print(f"Displayed comparison for image: {img_name}")
        print(f"Number of additional predictions: {len(additional_preds)}")

# Load your trained model
model = YOLO('/content/runs/detect/train2/weights/best.pt')

# Directory containing your test images and labels folders
test_dir = '/content/Villgro-8/test'

# Run the new function
highlight_additional_predictions(test_dir, model)
